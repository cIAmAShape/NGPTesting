# Prompt Engineering

Large Language Models (LLMs) like ChatGPT are powerful tools trained on massive amounts of text to predict and generate human-like language. They don’t "think" or "reason" the way humans do; instead, they statistically predict the next word in a sentence based on the input they’ve seen. <br />

Because LLMs respond based on patterns, *how you ask a question directly affects the quality of the answer*. This is why **prompt engineering** — the practice of crafting inputs to get the most effective outputs — is so important. It helps bridge the gap between your intention and what the model actually delivers. <br />

The main thing that you need to know: asking just one question is **rarely** enough to help. Unless you have a generic and simple problem — such as asking an LLM to remind you what the area of a sphere is — it likely won’t be a result applicable to your research. But don't fret! There are ways to get the results you are looking for.

### Basic Steps

You need to start with an *informed, reasonable question* that you have at least some basic research on. Then, you can ask the LLM a series of questions that (ideally) build on one another. Here is the way to go about that:
1. Use the LLM to suggest **reputable** reading material on your question or problem.
2. Based on what you learn from your reading, ask your first question
3. *Refine* your question based on the answer it gave you. 
4. Test the proposed *solution / insight* to the *issue / body of knowledge*, and once again refine your question. 
5. Continue this loop of refining and testing your query until the issue is resolved.

We will be going over each step in more detail and providing everyone with an exercise to apply this to.


## Exercise Requirements
